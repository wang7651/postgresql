"""
æœ¬åœ° LLM + MCP è‡ªç„¶èªè¨€æŸ¥è©¢ä»‹é¢
æ”¯æ´ä½¿ç”¨è‡ªç„¶èªè¨€æŸ¥è©¢è³‡æ–™åº«
"""

import asyncio
import json
import subprocess
from typing import Dict, Any, List
import re

# é¸æ“‡ä¸€å€‹æœ¬åœ° LLM æ¡†æ¶ï¼Œé€™è£¡ä»¥ Ollama ç‚ºä¾‹
import requests
from dataclasses import dataclass

@dataclass
class QueryResult:
    success: bool
    data: Any
    error: str = None

class MCPClient:
    """MCP å®¢æˆ¶ç«¯ï¼Œç”¨æ–¼èˆ‡è³‡æ–™åº« MCP server é€šè¨Š"""

    def __init__(self, command: str, args: List[str]):
        self.command = command
        self.args = args
        self.process = None
        self.request_id = 0

    async def start(self):
        """å•Ÿå‹• MCP server"""
        self.process = await asyncio.create_subprocess_exec(
            self.command, *self.args,
            stdin=asyncio.subprocess.PIPE,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )

        # åˆå§‹åŒ–é€£æ¥
        await self.initialize()

    async def send_request(self, method: str, params: Dict[str, Any] = None) -> Dict[str, Any]:
        if not self.process:
            raise RuntimeError("MCP Client not started")

        self.request_id += 1
        request = {
            "jsonrpc": "2.0",
            "id": self.request_id,
            "method": method,
            "params": params or {}
        }

        request_json = json.dumps(request) + '\n'
        self.process.stdin.write(request_json.encode())
        await self.process.stdin.drain()

        response_line = await self.process.stdout.readline()
        return json.loads(response_line.decode())

    async def initialize(self):
        return await self.send_request("initialize", {
            "protocolVersion": "2024-11-05",
            "capabilities": {"tools": {}},
            "clientInfo": {"name": "local-llm-client", "version": "1.0.0"}
        })

    async def list_tools(self) -> List[str]:
        response = await self.send_request("tools/list")
        if "result" in response and "tools" in response["result"]:
            return [tool["name"] for tool in response["result"]["tools"]]
        return []

    async def get_schema_info(self) -> str:
        """ç²å–è³‡æ–™åº« schema è³‡è¨Š"""
        try:
            # ç²å–è³‡æ–™è¡¨åˆ—è¡¨
            tables_result = await self.call_tool("query", {
                "sql": "SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';"
            })

            schema_info = "Database Schema:\n"
            if tables_result.success and tables_result.data:
                for row in tables_result.data:
                    table_name = row[0] if isinstance(row, (list, tuple)) else row.get('table_name')
                    schema_info += f"\nTable: {table_name}\n"

                    # ç²å–æ¯å€‹è¡¨çš„æ¬„ä½è³‡è¨Š
                    columns_result = await self.call_tool("query", {
                        "sql": f"""
                        SELECT column_name, data_type, is_nullable
                        FROM information_schema.columns
                        WHERE table_name = '{table_name}' AND table_schema = 'public'
                        ORDER BY ordinal_position;
                        """
                    })

                    if columns_result.success and columns_result.data:
                        for col_row in columns_result.data:
                            col_name = col_row[0] if isinstance(col_row, (list, tuple)) else col_row.get('column_name')
                            col_type = col_row[1] if isinstance(col_row, (list, tuple)) else col_row.get('data_type')
                            nullable = col_row[2] if isinstance(col_row, (list, tuple)) else col_row.get('is_nullable')
                            schema_info += f"  - {col_name}: {col_type} ({'nullable' if nullable == 'YES' else 'not null'})\n"

            return schema_info
        except Exception as e:
            return f"Error getting schema: {str(e)}"

    async def call_tool(self, name: str, arguments: Dict[str, Any]) -> QueryResult:
        try:
            response = await self.send_request("tools/call", {
                "name": name,
                "arguments": arguments
            })

            if "result" in response:
                return QueryResult(success=True, data=response["result"])
            else:
                return QueryResult(success=False, error=response.get("error", "Unknown error"))
        except Exception as e:
            return QueryResult(success=False, error=str(e))

    async def close(self):
        if self.process:
            self.process.terminate()
            await self.process.wait()

class LocalLLM:
    """æœ¬åœ° LLM ä»‹é¢ï¼Œé€™è£¡ä½¿ç”¨ Ollama ä½œç‚ºç¯„ä¾‹"""

    def __init__(self, model_name: str = "llama3.2:lastest", base_url: str = "http://localhost:11434"):
        self.model_name = model_name
        self.base_url = base_url

    def generate(self, prompt: str) -> str:
        """ä½¿ç”¨æœ¬åœ° LLM ç”Ÿæˆå›æ‡‰"""
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                return result.get("response", "").strip()
            else:
                return f"LLM Error: HTTP {response.status_code}"

        except requests.exceptions.RequestException as e:
            return f"LLM Connection Error: {str(e)}"

class NaturalLanguageQueryInterface:
    """è‡ªç„¶èªè¨€æŸ¥è©¢ä»‹é¢"""

    def __init__(self, mcp_client: MCPClient, llm: LocalLLM):
        self.mcp_client = mcp_client
        self.llm = llm
        self.schema_info = ""

    async def initialize(self):
        """åˆå§‹åŒ–ç³»çµ±"""
        await self.mcp_client.start()
        self.schema_info = await self.mcp_client.get_schema_info()
        print("âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š è³‡æ–™åº« Schema å·²è¼‰å…¥")

    def create_sql_generation_prompt(self, user_question: str) -> str:
        """å»ºç«‹ SQL ç”Ÿæˆçš„æç¤ºè©"""
        return f"""
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ SQL æŸ¥è©¢åŠ©æ‰‹ã€‚è«‹æ ¹æ“šç”¨æˆ¶çš„è‡ªç„¶èªè¨€å•é¡Œï¼Œç”Ÿæˆå°æ‡‰çš„ PostgreSQL SQL æŸ¥è©¢èªå¥ã€‚

è³‡æ–™åº«çµæ§‹ï¼š
{self.schema_info}

ç”¨æˆ¶å•é¡Œï¼š{user_question}

è«‹éµå¾ªä»¥ä¸‹è¦å‰‡ï¼š
1. åªè¿”å› SQL æŸ¥è©¢èªå¥ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡‹
2. ä½¿ç”¨æ¨™æº–çš„ PostgreSQL èªæ³•
3. å¦‚æœå•é¡Œæ¨¡ç³Šï¼Œé¸æ“‡æœ€åˆç†çš„è§£é‡‹
4. å¦‚æœéœ€è¦é™åˆ¶çµæœæ•¸é‡ï¼Œé è¨­ä½¿ç”¨ LIMIT 10
5. SQL èªå¥æ‡‰è©²ä»¥åˆ†è™Ÿçµå°¾

SQL æŸ¥è©¢ï¼š
"""

    def extract_sql(self, llm_response: str) -> str:
        """å¾ LLM å›æ‡‰ä¸­æå– SQL èªå¥"""
        # ç§»é™¤å¸¸è¦‹çš„åŒ…è£æ–‡å­—
        response = llm_response.strip()

        # å˜—è©¦æå– SQL ä»£ç¢¼å¡Š
        sql_pattern = r'```sql\s*(.*?)\s*```'
        match = re.search(sql_pattern, response, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()

        # å¦‚æœæ²’æœ‰ä»£ç¢¼å¡Šï¼Œå°‹æ‰¾ SELECT èªå¥
        select_pattern = r'(SELECT\s+.*?;)'
        match = re.search(select_pattern, response, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()

        # ç›´æ¥è¿”å›å›æ‡‰ï¼ˆå‡è¨­æ•´å€‹å›æ‡‰å°±æ˜¯ SQLï¼‰
        return response

    async def query(self, user_question: str) -> Dict[str, Any]:
        """è™•ç†è‡ªç„¶èªè¨€æŸ¥è©¢"""
        print(f"ğŸ¤” è™•ç†å•é¡Œ: {user_question}")

        # 1. ä½¿ç”¨ LLM ç”Ÿæˆ SQL
        sql_prompt = self.create_sql_generation_prompt(user_question)
        llm_response = self.llm.generate(sql_prompt)
        sql_query = self.extract_sql(llm_response)

        print(f"ğŸ” ç”Ÿæˆçš„ SQL: {sql_query}")

        # 2. åŸ·è¡Œ SQL æŸ¥è©¢
        result = await self.mcp_client.call_tool("query", {"sql": sql_query})

        if result.success:
            print(f"âœ… æŸ¥è©¢æˆåŠŸï¼Œè¿”å› {len(result.data) if result.data else 0} ç­†è¨˜éŒ„")
            return {
                "success": True,
                "question": user_question,
                "sql": sql_query,
                "data": result.data,
                "count": len(result.data) if result.data else 0
            }
        else:
            print(f"âŒ æŸ¥è©¢å¤±æ•—: {result.error}")
            return {
                "success": False,
                "question": user_question,
                "sql": sql_query,
                "error": result.error
            }

    async def close(self):
        """é—œé–‰é€£æ¥"""
        await self.mcp_client.close()

# ä¸»è¦åŸ·è¡Œå‡½æ•¸
async def main():
    # è¨­å®š MCP å®¢æˆ¶ç«¯ï¼ˆé€£æ¥åˆ° PostgreSQLï¼‰
    mcp_client = MCPClient("npx", [
        "-y",
        "@modelcontextprotocol/server-postgres",
        "postgresql://postgres:raspberry@host.docker.internal:5432/postgres"
    ])

    # è¨­å®šæœ¬åœ° LLMï¼ˆéœ€è¦å…ˆå•Ÿå‹• Ollamaï¼‰
    llm = LocalLLM(model_name="llama3.2:lastest")  # æˆ–ä½¿ç”¨å…¶ä»–æ¨¡å‹å¦‚ "codellama", "mistral" ç­‰

    # å»ºç«‹è‡ªç„¶èªè¨€æŸ¥è©¢ä»‹é¢
    interface = NaturalLanguageQueryInterface(mcp_client, llm)

    try:
        # åˆå§‹åŒ–
        await interface.initialize()

        print("\nğŸš€ è‡ªç„¶èªè¨€è³‡æ–™åº«æŸ¥è©¢ç³»çµ±å·²å•Ÿå‹•ï¼")
        print("ğŸ“ ä½ å¯ä»¥ç”¨è‡ªç„¶èªè¨€è©¢å•è³‡æ–™åº«å•é¡Œï¼Œä¾‹å¦‚ï¼š")
        print("   - 'é¡¯ç¤ºæ‰€æœ‰è³‡æ–™è¡¨'")
        print("   - 'æ‰¾å‡ºæœ€è¿‘ 10 ç­†è¨‚å–®'")
        print("   - 'çµ±è¨ˆæ¯å€‹åŸå¸‚çš„å®¢æˆ¶æ•¸é‡'")
        print("   - è¼¸å…¥ 'quit' çµæŸç¨‹å¼\n")

        # äº’å‹•å¼æŸ¥è©¢è¿´åœˆ
        while True:
            user_input = input("â“ è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ: ").strip()

            if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                break

            if not user_input:
                continue

            # è™•ç†æŸ¥è©¢
            result = await interface.query(user_input)

            if result["success"]:
                print(f"\nğŸ“Š æŸ¥è©¢çµæœ ({result['count']} ç­†è¨˜éŒ„):")
                if result["data"]:
                    # é¡¯ç¤ºå‰å¹¾ç­†è³‡æ–™
                    for i, row in enumerate(result["data"][:5]):  # åªé¡¯ç¤ºå‰ 5 ç­†
                        print(f"  {i+1}. {row}")

                    if len(result["data"]) > 5:
                        print(f"  ... é‚„æœ‰ {len(result['data']) - 5} ç­†è¨˜éŒ„")
                else:
                    print("  (ç„¡è³‡æ–™)")
            else:
                print(f"\nâŒ æŸ¥è©¢å¤±æ•—: {result['error']}")

            print("-" * 60)

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹å¼è¢«ä¸­æ–·")
    except Exception as e:
        print(f"\nğŸ’¥ ç™¼ç”ŸéŒ¯èª¤: {e}")
    finally:
        await interface.close()
        print("ğŸ”š é€£æ¥å·²é—œé–‰")

# æ›¿ä»£çš„ LLM å¯¦ä½œï¼ˆå¦‚æœä¸æƒ³ä½¿ç”¨ Ollamaï¼‰
class OpenAICompatibleLLM:
    """OpenAI ç›¸å®¹çš„ LLM ä»‹é¢ï¼ˆå¯ç”¨æ–¼æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹ï¼‰"""

    def __init__(self, base_url: str, api_key: str = "dummy", model: str = "gpt-3.5-turbo"):
        self.base_url = base_url
        self.api_key = api_key
        self.model = model

    def generate(self, prompt: str) -> str:
        try:
            response = requests.post(
                f"{self.base_url}/v1/chat/completions",
                headers={"Authorization": f"Bearer {self.api_key}"},
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 500,
                    "temperature": 0.1
                }
            )

            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                return f"API Error: {response.status_code}"

        except Exception as e:
            return f"Connection Error: {str(e)}"

if __name__ == "__main__":
    print("ğŸ¤– å•Ÿå‹•æœ¬åœ° LLM + MCP è‡ªç„¶èªè¨€æŸ¥è©¢ç³»çµ±...")
    print("âš ï¸  è«‹ç¢ºä¿ä»¥ä¸‹æœå‹™å·²å•Ÿå‹•ï¼š")
    print("   1. PostgreSQL è³‡æ–™åº«")
    print("   2. Ollama æœå‹™ (ollama serve)")
    print("   3. å·²ä¸‹è¼‰æ‰€éœ€çš„æ¨¡å‹ (ollama pull llama3.2:lastest)")
    print()

    asyncio.run(main())